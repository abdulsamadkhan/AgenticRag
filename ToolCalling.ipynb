{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulsamadkhan/AgenticRag/blob/main/ToolCalling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic RAG Application using LlamaIndex - Tool Calling\n",
        "\n",
        "In the last tutorial, we learnt how to use LLM to pick one of the RAG pipelines using RouterQueryEngine. In this tutorial, we will see how LLM can not only pick the function or the RAG pipeline to execute but also infer the arguments of the function or the arguments of the pipeline. so, the last tutorial was a simple case of tool calling and in this tutorial, we will use tool calling with arguments.\n",
        "\n",
        "Note that Tool Calling adds a layer of understanding on the top of RAG pipeline which enables users to ask complex query and get back more precise answers.\n"
      ],
      "metadata": {
        "id": "5uzSvWrf3Ns7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tool Calling\n",
        "Tool calling enables LLM to interact with the external environment through a dynamic interface. This interface not only helps in selecting the appropriate function but also infers the necessary arguments for execution, this is called Tool Calling."
      ],
      "metadata": {
        "id": "aHoEcNQ_FoUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Requirements\n",
        "**installing libraries:**This will install the necessary libraries."
      ],
      "metadata": {
        "id": "XvBSzyOohOLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index==0.10.27\n",
        "!pip install llama-index-llms-openai==0.1.15\n",
        "!pip install llama-index-embeddings-openai==0.1.7"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FbX7g6HAhNvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weâ€™ll also use the nest-asyncio library since Llama-index uses a lot of asyncio functionality in the background"
      ],
      "metadata": {
        "id": "Ahk3uHj-_5Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "IlWMN6Jp_9FC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting OpenAI Key:**Load the Open API Key, in the colab environment."
      ],
      "metadata": {
        "id": "bItbOShKvfkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB7vwMSNiqI3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#loading openAI API Key\n",
        "!pip install openai\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_KEY')\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=openai.api_key,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a function Tool\n",
        "A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema.\n",
        "We will demonstrate the idea of a function tool through a simple example. We will define two toy functions `add` and `mystery`. also note that both functions have type annotation and doc string. both the type annotations and doc string are important because they will be used as a prompt for LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "bs0PE_chgKR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"\"\"Adds two integers together.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "def mystery(x: int, y: int) -> int:\n",
        "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
        "    return (x + y) * (x + y)\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
      ],
      "metadata": {
        "id": "uMUg6MM0ijbi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function tool natively integrates with the function calling capabilities of many LLMs including openAI. The predict_and_call function of the LLM takes a set of function tools and the query and then calls the right tool with parameters and returns the results."
      ],
      "metadata": {
        "id": "C8V-0Gqem5eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "response = llm.predict_and_call(\n",
        "    [add_tool, mystery_tool],\n",
        "    \"Tell me the output of the mystery function on 2 and 9\",\n",
        "    verbose=True\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cavhw7vQlcKm",
        "outputId": "2589d9e3-80a5-4ce5-fc50-2f5b764c2fbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
            "=== Function Output ===\n",
            "121\n",
            "121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the output we see that LLM called the right tool with parameters, this is an extended example of the router, where not only is the right tool picked but also the parameters are inferred."
      ],
      "metadata": {
        "id": "clGLOWmQnu0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vector Search with Metadata\n",
        "We can not only choose the vector search but also infer the metadata filters. These filters will help us return more precise results. Let's experiment with a PDF file and use the metadata for inferring a particular information.\n",
        "\n",
        "**Download PDF** you can change the code to download the pdf of your interest."
      ],
      "metadata": {
        "id": "IiFO3-fl2uZ_"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "\n",
        "# Define the URL and filename\n",
        "url = \"https://arxiv.org/pdf/2308.00352\"\n",
        "filename = \"MetaGPT.pdf\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check for successful response\n",
        "if response.status_code == 200:\n",
        "    # Open the file in write binary mode\n",
        "    with open(filename, \"wb\") as output_file:\n",
        "        # Write the content of the response to the file\n",
        "        output_file.write(response.content)\n",
        "    print(\"Successfully downloaded the PDF file\")\n",
        "else:\n",
        "    print(\"Error: Failed to download the PDF file\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4CEPk3s4iHRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab2394d-3b4e-4da7-d092-a2566bf45488"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded the PDF file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading data and creating chunks**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_zbcd9xDjP9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"MetaGPT.pdf\"]).load_data()\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "4MAC-jFLhlO0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at the content of the first chunk."
      ],
      "metadata": {
        "id": "HOyE3-pr8R6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes[0].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "id": "ZZ_BUFopFCuB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the page_label durin the filtering"
      ],
      "metadata": {
        "id": "ONTqbBqCIor9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creation of  Vector Index**\n",
        "\n",
        "The below lines of code will create a vector index for the given list of nodes"
      ],
      "metadata": {
        "id": "t6st6xEwkzL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import  VectorStoreIndex\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "HbwMEYuskuOw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can query this RAG pipeline using a metadata filter."
      ],
      "metadata": {
        "id": "ZdTJkbKTK-Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters\n",
        "\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    filters=MetadataFilters.from_dicts(\n",
        "        [\n",
        "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"What are some high-level results of MetaGPT?\",\n",
        ")"
      ],
      "metadata": {
        "id": "yAi8lE3Mk3da"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the source nodes"
      ],
      "metadata": {
        "id": "vpiTj59yL2kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gl-KnPOL8RF",
        "outputId": "d6d36db1-cd08-4505-c938-64e50b736852"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '2', 'file_name': 'MetaGPT.pdf', 'file_path': 'MetaGPT.pdf', 'file_type': 'application/pdf', 'file_size': 16715764, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the research is only restricted to page number 2."
      ],
      "metadata": {
        "id": "TbsqkQHvMEgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval tool\n",
        "We can integrate this MetaData filters into a retrieval tool function. This function takes the query string and optional metadata filters such as page number. The LLM can intelligently infer the relevant metadata filter (e.g., page number) based on the use's query. We can define different type of metadata filters like section IDS, Headers etc.\n",
        "Let us define a function called vector query, which takes in the query and page number. This allows to perform a vector search over an index along with specifying a page number as a metadata filter.\n"
      ],
      "metadata": {
        "id": "Q7NAQ9iVsv0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from llama_index.core.vector_stores import FilterCondition\n",
        "\n",
        "\n",
        "def vector_query(\n",
        "    query: str,\n",
        "    page_numbers: List[str]\n",
        ") -> str:\n",
        "    \"\"\"Perform a vector search over an index.\n",
        "\n",
        "    query (str): the string query to be embedded.\n",
        "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
        "        over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metadata_dicts = [\n",
        "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "    ]\n",
        "\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=2,\n",
        "        filters=MetadataFilters.from_dicts(\n",
        "            metadata_dicts,\n",
        "            condition=FilterCondition.OR\n",
        "        )\n",
        "    )\n",
        "    response = query_engine.query(query)\n",
        "    return response\n",
        "\n",
        "\n",
        "vector_query_tool = FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        ")"
      ],
      "metadata": {
        "id": "pCS0JhH_lIrN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define a vector query tool as function tool. We pass the vector query function into a vector query tool, which allows us to use it with the language model."
      ],
      "metadata": {
        "id": "LHm2u3jmUraR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_query_tool = FunctionTool.from_defaults(\n",
        "    name=\"vector_tool\",\n",
        "    fn=vector_query\n",
        ")"
      ],
      "metadata": {
        "id": "E76_FVT1V9yb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool],\n",
        "    \"What are the high-level results of MetaGPT as described on page 2?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "lASE5j_gX1nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcc61ae-8fda-48e9-f3e9-d7b1c8058ff1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT achieves a new state-of-the-art in code generation benchmarks with 85.9% and 87.7% in Pass@1. It outperforms other popular frameworks for creating complex software projects and stands out in handling higher levels of software complexity while offering extensive functionality. In experimental evaluations, MetaGPT demonstrates a 100% task completion rate, showcasing its robustness and efficiency in terms of time and token costs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the output that LLM successfully called the vector_query_tool and infered the argument i.e., page_no  and later printed the output."
      ],
      "metadata": {
        "id": "isDLT_iFd4Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "Kp5PxjAuXEMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b264c50-0eee-47db-df15-4ef7b593e285"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaGPT achieves a new state-of-the-art in code generation benchmarks with 85.9% and 87.7% in Pass@1. It outperforms other popular frameworks for creating complex software projects and stands out in handling higher levels of software complexity while offering extensive functionality. In experimental evaluations, MetaGPT demonstrates a 100% task completion rate, showcasing its robustness and efficiency in terms of time and token costs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us inspect the source of the response and we will see that LLM generated the output from the page number specified in the query."
      ],
      "metadata": {
        "id": "a6HVtKefed0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "id": "o0m9N-gaXLLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d123e04-1738-4652-d61a-5994598c29b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '2', 'file_name': 'MetaGPT.pdf', 'file_path': 'MetaGPT.pdf', 'file_type': 'application/pdf', 'file_size': 16715764, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us add summary_index_tool along with the existing vector_query_tool to test in more complex scenarios."
      ],
      "metadata": {
        "id": "z5UBi9UQe4D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    name=\"summary_tool\",\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful if you want to get a summary of MetaGPT\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "twG2ecBzXju7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "T1w4IaOPXf3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06dd6ccd-915f-4761-f108-2ff9debdafda"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
            "=== Function Output ===\n",
            "MetaGPT demonstrates higher executability, lower running times, and lower token usage compared to ChatDev. Additionally, MetaGPT has more code files, more lines of code per file, and more total code lines than ChatDev. In terms of productivity, MetaGPT outperforms ChatDev. However, ChatDev shows lower human revision cost compared to MetaGPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    print(n.metadata)"
      ],
      "metadata": {
        "id": "rAcpp6PgXW4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0858311-7bfd-4aa9-af18-5ad3b0f3f238"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page_label': '8', 'file_name': 'MetaGPT.pdf', 'file_path': 'MetaGPT.pdf', 'file_type': 'application/pdf', 'file_size': 16715764, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output we can see that LLM was able to successfully call the related function with infered page ino."
      ],
      "metadata": {
        "id": "k1MY6HXwfW8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict_and_call(\n",
        "    [vector_query_tool, summary_tool],\n",
        "    \"What is a summary of the paper?\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "HHw2dSnRXTr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ff4b04-172f-4158-fc60-715cfecac55f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: summary_tool with args: {\"input\": \"Please provide a summary of the paper.\"}\n",
            "=== Function Output ===\n",
            "The paper introduces MetaGPT, a meta-programming framework for LLM-based multi-agent systems that enhances problem-solving processes through collaborative interactions. MetaGPT incorporates Standard Operating Procedures (SOPs) to streamline workflows, assign roles to agents, and ensure structured communication. By leveraging human-like domain expertise, MetaGPT improves the quality and coherence of solutions generated by multi-agent collaborations. The framework achieves state-of-the-art performance in code generation benchmarks and offers a robust and efficient platform for developing LLM-based multi-agent systems. Additionally, MetaGPT focuses on recursive self-improvement mechanisms, multi-agent economies in software development, and a structured approach to transforming abstract requirements into detailed designs, showcasing its effectiveness in generating executable code and simplifying complex system development through specialized workflows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "* https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex\n"
      ],
      "metadata": {
        "id": "UqvXYcbvqkNr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsG9qk8cqkvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}